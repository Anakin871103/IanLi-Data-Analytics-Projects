{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 鐵達尼號生還預測：羅吉斯迴歸模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境準備\n",
    "導入所需的套件，並從我們建立的 `data_preprocessing.py` 腳本中導入資料準備函式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "\n",
    "# 設定中文字體和圖表樣式\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(cleaned_data_path, features_to_drop, target_column):\n",
    "    \"\"\"\n",
    "    從清理後的訓練數據載入資料，進行特徵編碼，為模型訓練做準備。\n",
    "\n",
    "    Args:\n",
    "        cleaned_data_path (str): train_cleaned.csv 檔案的路徑。\n",
    "        features_to_drop (list): 需要從特徵集中移除的欄位列表。\n",
    "        target_column (str): 目標變數的欄位名稱。\n",
    "\n",
    "    Returns:\n",
    "        tuple: 回傳 (X_train, y_train, feature_columns)\n",
    "        X_train 訓練用的特徵\n",
    "        y_train 訓練用的答案\n",
    "        feature_columns 特徵欄位名稱列表\n",
    "    \"\"\"\n",
    "    # 載入清理後的資料\n",
    "    df = pd.read_csv(cleaned_data_path)\n",
    "\n",
    "    # --- 特徵編碼 ---\n",
    "    # 對指定的類別欄位進行 One-Hot Encoding\n",
    "    categorical_features = ['Pclass', 'Sex', 'Embarked', 'Title', 'Age_Group']\n",
    "    df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
    "    \n",
    "    # --- 定義特徵(X)與目標(y) ---\n",
    "    y_train = df_encoded[target_column]\n",
    "    X_train = df_encoded.drop(columns=[target_column] + features_to_drop, axis=1)\n",
    "    \n",
    "    # 確保所有特徵都是數值型態\n",
    "    X_train = X_train.apply(pd.to_numeric)\n",
    "    \n",
    "    # 記錄特徵欄位名稱，用於後續測試資料的對齊\n",
    "    feature_columns = X_train.columns.tolist()\n",
    "\n",
    "    return X_train, y_train, feature_columns\n",
    "\n",
    "\n",
    "def prepare_test_data(test_data_path, features_to_drop, feature_columns):\n",
    "    \"\"\"\n",
    "    從外部測試資料載入並處理資料，確保與訓練資料的特徵一致。\n",
    "\n",
    "    Args:\n",
    "        test_data_path (str): test.csv 檔案的路徑。\n",
    "        features_to_drop (list): 需要從特徵集中移除的欄位列表。\n",
    "        feature_columns (list): 訓練時使用的特徵欄位名稱列表。\n",
    "\n",
    "    Returns:\n",
    "        tuple: 回傳 (X_test, y_test, test_df)\n",
    "        X_test 測試用的特徵\n",
    "        y_test 測試用的答案（如果存在）\n",
    "        test_df 原始測試資料框（用於分析）\n",
    "    \"\"\"\n",
    "    # 載入測試資料\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    \n",
    "    # 檢查是否有Survived欄位（有些測試集可能沒有答案）\n",
    "    has_target = 'Survived' in test_df.columns\n",
    "    \n",
    "    # --- 特徵編碼 ---\n",
    "    categorical_features = ['Pclass', 'Sex', 'Embarked', 'Title', 'Age_Group']\n",
    "    test_encoded = pd.get_dummies(test_df, columns=categorical_features, drop_first=True)\n",
    "    \n",
    "    # 移除不需要的欄位\n",
    "    columns_to_drop = features_to_drop.copy()\n",
    "    if has_target:\n",
    "        columns_to_drop.append('Survived')\n",
    "    \n",
    "    X_test = test_encoded.drop(columns=[col for col in columns_to_drop if col in test_encoded.columns], axis=1)\n",
    "    \n",
    "    # 確保測試資料的特徵與訓練資料一致\n",
    "    # 添加缺失的特徵欄位（設為0）\n",
    "    for col in feature_columns:\n",
    "        if col not in X_test.columns:\n",
    "            X_test[col] = 0\n",
    "    \n",
    "    # 移除多餘的特徵欄位\n",
    "    X_test = X_test[feature_columns]\n",
    "    \n",
    "    # 確保所有特徵都是數值型態\n",
    "    X_test = X_test.apply(pd.to_numeric)\n",
    "    \n",
    "    # 如果有目標變數，則提取\n",
    "    y_test = test_encoded['Survived'] if has_target else None\n",
    "\n",
    "    return X_test, y_test, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 資料準備\n",
    "呼叫 `prepare_data_for_modeling` 函式來載入資料、進行特徵編碼和分割，為模型訓練做好準備。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'fsspec'.  Use pip or conda to install fsspec.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jedi8\\anaconda3\\envs\\AnalysisProject\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jedi8\\anaconda3\\envs\\AnalysisProject\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'fsspec'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     10\u001b[39m X_train, y_train, feature_columns = prepare_training_data(\n\u001b[32m     11\u001b[39m     train_cleaned_path,\n\u001b[32m     12\u001b[39m     features_to_drop=features_to_drop,\n\u001b[32m     13\u001b[39m     target_column=target_column\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 準備測試資料\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m X_test, y_test, test_df = \u001b[43mprepare_test_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_data_path\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC://Users//jedi8//Documents//GitHub//IanLi-Data-Analytics-Projects//projects//01_Exploratory_Data_Analysis//001_EDA_Project_A_Titanic_Survival_Analysis//data//test.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures_to_drop\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_to_drop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_columns\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m資料準備完成！\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m訓練集特徵數量: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mprepare_test_data\u001b[39m\u001b[34m(test_data_path, features_to_drop, feature_columns)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m從外部測試資料載入並處理資料，確保與訓練資料的特徵一致。\u001b[39;00m\n\u001b[32m     40\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    test_df 原始測試資料框（用於分析）\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# 載入測試資料\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m test_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# 檢查是否有Survived欄位（有些測試集可能沒有答案）\u001b[39;00m\n\u001b[32m     56\u001b[39m has_target = \u001b[33m'\u001b[39m\u001b[33mSurvived\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m test_df.columns\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jedi8\\anaconda3\\envs\\AnalysisProject\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jedi8\\anaconda3\\envs\\AnalysisProject\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jedi8\\anaconda3\\envs\\AnalysisProject\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jedi8\\anaconda3\\envs\\AnalysisProject\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jedi8\\anaconda3\\envs\\AnalysisProject\\Lib\\site-packages\\pandas\\io\\common.py:728\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    725\u001b[39m     codecs.lookup_error(errors)\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ioargs = \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m handle = ioargs.filepath_or_buffer\n\u001b[32m    737\u001b[39m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jedi8\\anaconda3\\envs\\AnalysisProject\\Lib\\site-packages\\pandas\\io\\common.py:409\u001b[39m, in \u001b[36m_get_filepath_or_buffer\u001b[39m\u001b[34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[39m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filepath_or_buffer.startswith(\u001b[33m\"\u001b[39m\u001b[33ms3n://\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    408\u001b[39m     filepath_or_buffer = filepath_or_buffer.replace(\u001b[33m\"\u001b[39m\u001b[33ms3n://\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33ms3://\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m fsspec = \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfsspec\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[38;5;66;03m# If botocore is installed we fallback to reading with anon=True\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[38;5;66;03m# to allow reads from public buckets\u001b[39;00m\n\u001b[32m    413\u001b[39m err_types_to_retry_with_anon: \u001b[38;5;28mlist\u001b[39m[Any] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jedi8\\anaconda3\\envs\\AnalysisProject\\Lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Missing optional dependency 'fsspec'.  Use pip or conda to install fsspec."
     ]
    }
   ],
   "source": [
    "# 定義資料檔案路徑\n",
    "train_cleaned_path = '../data/train_cleaned.csv'\n",
    "test_data_path = r'C:\\Users\\jedi8\\Documents\\GitHub\\IanLi-Data-Analytics-Projects\\projects\\01_Exploratory_Data_Analysis\\001_EDA_Project_A_Titanic_Survival_Analysis\\data\\test.csv'\n",
    "target_column = 'Survived'\n",
    "\n",
    "# 這些是不具預測性的 ID 或已被轉換/合併的原始欄位\n",
    "features_to_drop = ['PassengerId', 'Name', 'Ticket', 'SibSp', 'Parch', 'Fare', 'Age']\n",
    "\n",
    "# 準備訓練資料\n",
    "X_train, y_train, feature_columns = prepare_training_data(\n",
    "    train_cleaned_path,\n",
    "    features_to_drop=features_to_drop,\n",
    "    target_column=target_column\n",
    ")\n",
    "\n",
    "# 準備測試資料\n",
    "X_test, y_test, test_df = prepare_test_data(\n",
    "    test_data_path   = \"C:\\\\Users\\\\jedi8\\\\Documents\\\\GitHub\\\\IanLi-Data-Analytics-Projects\\\\projects\\\\01_Exploratory_Data_Analysis\\\\001_EDA_Project_A_Titanic_Survival_Analysis\\\\data\\\\test.csv\",\n",
    "    features_to_drop = features_to_drop,\n",
    "    feature_columns  = feature_columns\n",
    ")\n",
    "\n",
    "print(\"資料準備完成！\")\n",
    "print(f\"訓練集特徵數量: {X_train.shape[1]}\")\n",
    "print(f\"訓練集樣本數: {X_train.shape[0]}\")\n",
    "print(f\"測試集樣本數: {X_test.shape[0]}\")\n",
    "print(f\"測試集是否有答案: {'是' if y_test is not None else '否'}\")\n",
    "\n",
    "# 顯示特徵欄位\n",
    "print(f\"\\n使用的特徵欄位 ({len(feature_columns)}個):\")\n",
    "for i, col in enumerate(feature_columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型訓練與評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化羅吉斯迴歸模型\n",
    "# max_iter 增加迭代次數以確保收斂，random_state 確保結果可重現\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# 使用訓練集進行模型訓練\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# 在測試集上進行預測\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "print(\"模型訓練與預測完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評估模型效能\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"模型準確率 (Accuracy): {accuracy:.4f}\")\n",
    "print(\"混淆矩陣 (Confusion Matrix):\")\n",
    "print(conf_matrix)\n",
    "print(\"分類報告 (Classification Report):\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 外部測試資料預測結果\n",
    "使用訓練好的模型對外部測試資料進行預測。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對外部測試資料進行預測\n",
    "y_pred_test = log_reg.predict(X_test)\n",
    "y_pred_proba_test = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"外部測試資料預測完成！\")\n",
    "print(f\"預測為生還的乘客數: {np.sum(y_pred_test)}\")\n",
    "print(f\"預測為未生還的乘客數: {len(y_pred_test) - np.sum(y_pred_test)}\")\n",
    "print(f\"生還率預測: {np.mean(y_pred_test):.4f}\")\n",
    "\n",
    "# 顯示預測機率分布\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y_pred_proba_test, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('預測生還機率')\n",
    "plt.ylabel('乘客數量')\n",
    "plt.title('外部測試資料 - 預測機率分布')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "survival_counts = pd.Series(y_pred_test).value_counts().sort_index()\n",
    "plt.bar(['未生還', '生還'], survival_counts.values, color=['red', 'green'], alpha=0.7)\n",
    "plt.ylabel('乘客數量')\n",
    "plt.title('外部測試資料 - 預測結果分布')\n",
    "for i, v in enumerate(survival_counts.values):\n",
    "    plt.text(i, v + 5, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 如果測試資料有真實答案，則進行ROC分析\n",
    "if y_test is not None:\n",
    "    print(\"\\n=== ROC曲線分析 ===\")\n",
    "    print(\"測試資料包含真實答案，進行ROC分析...\")\n",
    "    \n",
    "    # 計算ROC曲線的數據點\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_test)\n",
    "    \n",
    "    # 計算AUC值\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # 計算測試集上的準確率\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    test_conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"測試集準確率: {test_accuracy:.4f}\")\n",
    "    print(f\"AUC值: {roc_auc:.4f}\")\n",
    "    print(\"測試集混淆矩陣:\")\n",
    "    print(test_conf_matrix)\n",
    "else:\n",
    "    print(\"\\n=== 注意 ===\")\n",
    "    print(\"測試資料不包含真實答案，無法進行ROC分析\")\n",
    "    print(\"僅顯示預測結果\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只有當測試資料有真實答案時才繪製ROC曲線\n",
    "if y_test is not None:\n",
    "    # 繪製ROC曲線\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # 繪製ROC曲線\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC曲線 (AUC = {roc_auc:.4f})')\n",
    "    \n",
    "    # 繪製對角線（隨機分類器的基準線）\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "             label='隨機分類器 (AUC = 0.5)')\n",
    "    \n",
    "    # 設定圖表屬性\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('偽陽性率 (False Positive Rate)', fontsize=12)\n",
    "    plt.ylabel('真陽性率 (True Positive Rate)', fontsize=12)\n",
    "    plt.title('羅吉斯迴歸模型 - ROC曲線 (外部測試資料)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\", fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 添加AUC值的文字說明\n",
    "    plt.text(0.6, 0.2, f'AUC = {roc_auc:.4f}\\n\\n解釋:\\n• AUC > 0.9: 優秀\\n• AUC > 0.8: 良好\\n• AUC > 0.7: 尚可\\n• AUC > 0.6: 差\\n• AUC ≤ 0.5: 無效', \n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7),\n",
    "             fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n由於測試資料沒有真實答案，無法繪製ROC曲線\")\n",
    "    print(\"如需ROC分析，請使用包含Survived欄位的測試資料\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 詳細分析（僅當測試資料有答案時）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只有當測試資料有真實答案時才進行詳細分析\n",
    "if y_test is not None:\n",
    "    # 創建更詳細的ROC分析圖表\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. ROC曲線\n",
    "    axes[0, 0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC曲線 (AUC = {roc_auc:.4f})')\n",
    "    axes[0, 0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='隨機分類器')\n",
    "    axes[0, 0].set_xlim([0.0, 1.0])\n",
    "    axes[0, 0].set_ylim([0.0, 1.05])\n",
    "    axes[0, 0].set_xlabel('偽陽性率 (FPR)')\n",
    "    axes[0, 0].set_ylabel('真陽性率 (TPR)')\n",
    "    axes[0, 0].set_title('ROC曲線')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. 預測機率分布\n",
    "    survived_proba = y_pred_proba_test[y_test == 1]\n",
    "    not_survived_proba = y_pred_proba_test[y_test == 0]\n",
    "    \n",
    "    axes[0, 1].hist(not_survived_proba, bins=30, alpha=0.7, label='未生還', color='red', density=True)\n",
    "    axes[0, 1].hist(survived_proba, bins=30, alpha=0.7, label='生還', color='green', density=True)\n",
    "    axes[0, 1].set_xlabel('預測機率')\n",
    "    axes[0, 1].set_ylabel('密度')\n",
    "    axes[0, 1].set_title('預測機率分布（按真實結果分組）')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. 閾值 vs 精確率/召回率\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_proba_test)\n",
    "    \n",
    "    # 為了與ROC的閾值對應，我們需要計算每個閾值下的精確率和召回率\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred_threshold = (y_pred_proba_test >= threshold).astype(int)\n",
    "        if len(np.unique(y_pred_threshold)) > 1:  # 避免除零錯誤\n",
    "            from sklearn.metrics import precision_score, recall_score\n",
    "            prec = precision_score(y_test, y_pred_threshold, zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred_threshold, zero_division=0)\n",
    "        else:\n",
    "            prec = 0\n",
    "            rec = 0\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "    \n",
    "    axes[1, 0].plot(thresholds, precisions, label='精確率 (Precision)', color='blue')\n",
    "    axes[1, 0].plot(thresholds, recalls, label='召回率 (Recall)', color='red')\n",
    "    axes[1, 0].set_xlabel('閾值')\n",
    "    axes[1, 0].set_ylabel('分數')\n",
    "    axes[1, 0].set_title('閾值 vs 精確率/召回率')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. 混淆矩陣熱力圖\n",
    "    sns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['未生還', '生還'], \n",
    "                yticklabels=['未生還', '生還'],\n",
    "                ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('測試集混淆矩陣')\n",
    "    axes[1, 1].set_xlabel('預測值')\n",
    "    axes[1, 1].set_ylabel('實際值')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"測試資料沒有真實答案，跳過詳細ROC分析\")\n",
    "    \n",
    "    # 顯示預測結果的詳細統計\n",
    "    print(\"\\n=== 預測結果統計 ===\")\n",
    "    print(f\"總乘客數: {len(y_pred_test)}\")\n",
    "    print(f\"預測生還: {np.sum(y_pred_test)} 人 ({np.mean(y_pred_test)*100:.1f}%)\")\n",
    "    print(f\"預測未生還: {len(y_pred_test) - np.sum(y_pred_test)} 人 ({(1-np.mean(y_pred_test))*100:.1f}%)\")\n",
    "    \n",
    "    # 顯示預測機率的統計\n",
    "    print(f\"\\n=== 預測機率統計 ===\")\n",
    "    print(f\"平均預測機率: {np.mean(y_pred_proba_test):.4f}\")\n",
    "    print(f\"機率標準差: {np.std(y_pred_proba_test):.4f}\")\n",
    "    print(f\"最高預測機率: {np.max(y_pred_proba_test):.4f}\")\n",
    "    print(f\"最低預測機率: {np.min(y_pred_proba_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型效能總結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"羅吉斯迴歸模型效能總結\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 訓練集效能\n",
    "print(\"\\n【訓練集效能】\")\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "train_precision = precision_score(y_test, y_pred)\n",
    "train_recall = recall_score(y_test, y_pred)\n",
    "train_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"準確率 (Accuracy):    {accuracy:.4f}\")\n",
    "print(f\"精確率 (Precision):   {train_precision:.4f}\")\n",
    "print(f\"召回率 (Recall):      {train_recall:.4f}\")\n",
    "print(f\"F1分數 (F1-Score):    {train_f1:.4f}\")\n",
    "\n",
    "# 外部測試集效能（如果有答案）\n",
    "if y_test is not None:\n",
    "    print(\"\\n【外部測試集效能】\")\n",
    "    test_precision = precision_score(y_test, y_pred_test)\n",
    "    test_recall = recall_score(y_test, y_pred_test)\n",
    "    test_f1 = f1_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"準確率 (Accuracy):    {test_accuracy:.4f}\")\n",
    "    print(f\"精確率 (Precision):   {test_precision:.4f}\")\n",
    "    print(f\"召回率 (Recall):      {test_recall:.4f}\")\n",
    "    print(f\"F1分數 (F1-Score):    {test_f1:.4f}\")\n",
    "    print(f\"AUC值:               {roc_auc:.4f}\")\n",
    "    \n",
    "    # ROC曲線解釋\n",
    "    print(\"\\n【ROC曲線分析】\")\n",
    "    print(f\"• AUC值為 {roc_auc:.4f}，\", end=\"\")\n",
    "    if roc_auc > 0.9:\n",
    "        print(\"模型效能優秀\")\n",
    "    elif roc_auc > 0.8:\n",
    "        print(\"模型效能良好\")\n",
    "    elif roc_auc > 0.7:\n",
    "        print(\"模型效能尚可\")\n",
    "    elif roc_auc > 0.6:\n",
    "        print(\"模型效能較差\")\n",
    "    else:\n",
    "        print(\"模型幾乎無預測能力\")\n",
    "    \n",
    "    print(f\"• 模型在區分生還與未生還乘客方面有 {(roc_auc-0.5)*200:.1f}% 的改善（相較於隨機猜測）\")\n",
    "    \n",
    "    # 找到最佳閾值（Youden's J statistic）\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    print(f\"• 建議的最佳分類閾值: {optimal_threshold:.4f}\")\n",
    "    print(f\"  在此閾值下，TPR = {tpr[optimal_idx]:.4f}, FPR = {fpr[optimal_idx]:.4f}\")\n",
    "else:\n",
    "    print(\"\\n【外部測試集預測】\")\n",
    "    print(f\"總預測樣本數:        {len(y_pred_test)}\")\n",
    "    print(f\"預測生還人數:        {np.sum(y_pred_test)}\")\n",
    "    print(f\"預測生還率:          {np.mean(y_pred_test):.4f}\")\n",
    "    print(f\"平均預測機率:        {np.mean(y_pred_proba_test):.4f}\")\n",
    "    print(\"\\n注意: 由於測試資料無真實答案，無法計算準確率和AUC值\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 保存預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建預測結果的DataFrame\n",
    "if 'PassengerId' in test_df.columns:\n",
    "    results_df = pd.DataFrame({\n",
    "        'PassengerId': test_df['PassengerId'],\n",
    "        'Survived': y_pred_test,\n",
    "        'Survival_Probability': y_pred_proba_test\n",
    "    })\n",
    "else:\n",
    "    results_df = pd.DataFrame({\n",
    "        'Index': range(len(y_pred_test)),\n",
    "        'Survived': y_pred_test,\n",
    "        'Survival_Probability': y_pred_proba_test\n",
    "    })\n",
    "\n",
    "# 顯示前10筆預測結果\n",
    "print(\"前10筆預測結果:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "# 保存預測結果到CSV檔案\n",
    "output_path = '../data/predictions_logistic_regression.csv'\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"\\n預測結果已保存至: {output_path}\")\n",
    "\n",
    "# 顯示預測結果統計\n",
    "print(f\"\\n預測結果統計:\")\n",
    "print(f\"總樣本數: {len(results_df)}\")\n",
    "print(f\"預測生還: {results_df['Survived'].sum()} 人\")\n",
    "print(f\"預測未生還: {len(results_df) - results_df['Survived'].sum()} 人\")\n",
    "print(f\"生還率: {results_df['Survived'].mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AnalysisProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
