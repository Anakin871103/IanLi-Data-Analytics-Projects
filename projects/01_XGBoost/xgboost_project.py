# XGBoost å°ˆæ¡ˆ - å®Œæ•´æ©Ÿå™¨å­¸ç¿’æµç¨‹
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import load_breast_cancer
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# è¨­å®šä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

class XGBoostProject:
    def __init__(self):
        self.model = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = StandardScaler()
        
    def load_data(self):
        """è¼‰å…¥è³‡æ–™é›† - ä½¿ç”¨ä¹³ç™Œè³‡æ–™é›†ä½œç‚ºç¯„ä¾‹"""
        print("æ­¥é©Ÿ 1: è¼‰å…¥è³‡æ–™é›†")
        print("=" * 50)
        
        # è¼‰å…¥ä¹³ç™Œè³‡æ–™é›†
        data = load_breast_cancer()
        self.df = pd.DataFrame(data.data, columns=data.feature_names)
        self.df['target'] = data.target
        
        print(f"è³‡æ–™é›†å½¢ç‹€: {self.df.shape}")
        print(f"ç‰¹å¾µæ•¸é‡: {len(data.feature_names)}")
        print(f"é¡åˆ¥åˆ†å¸ƒ:")
        print(self.df['target'].value_counts())
        print("\nå‰5ç­†è³‡æ–™:")
        print(self.df.head())
        
        return self.df
    
    def explore_data(self):
        """è³‡æ–™æ¢ç´¢èˆ‡è¦–è¦ºåŒ–"""
        print("\næ­¥é©Ÿ 2: è³‡æ–™æ¢ç´¢èˆ‡è¦–è¦ºåŒ–")
        print("=" * 50)
        
        # åŸºæœ¬çµ±è¨ˆè³‡è¨Š
        print("åŸºæœ¬çµ±è¨ˆè³‡è¨Š:")
        print(self.df.describe())
        
        # æª¢æŸ¥ç¼ºå¤±å€¼
        print(f"\nç¼ºå¤±å€¼æ•¸é‡: {self.df.isnull().sum().sum()}")
        
        # è¦–è¦ºåŒ–
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # ç›®æ¨™è®Šæ•¸åˆ†å¸ƒ
        self.df['target'].value_counts().plot(kind='bar', ax=axes[0,0])
        axes[0,0].set_title('ç›®æ¨™è®Šæ•¸åˆ†å¸ƒ')
        axes[0,0].set_xlabel('é¡åˆ¥ (0: æƒ¡æ€§, 1: è‰¯æ€§)')
        
        # ç‰¹å¾µç›¸é—œæ€§ç†±åŠ›åœ– (é¸æ“‡å‰10å€‹ç‰¹å¾µ)
        corr_matrix = self.df.iloc[:, :10].corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=axes[0,1])
        axes[0,1].set_title('ç‰¹å¾µç›¸é—œæ€§ç†±åŠ›åœ– (å‰10å€‹ç‰¹å¾µ)')
        
        # ç‰¹å¾µåˆ†å¸ƒ (é¸æ“‡å¹¾å€‹é‡è¦ç‰¹å¾µ)
        important_features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area']
        for i, feature in enumerate(important_features[:2]):
            axes[1,i].hist(self.df[feature], bins=30, alpha=0.7)
            axes[1,i].set_title(f'{feature} åˆ†å¸ƒ')
        
        plt.tight_layout()
        plt.savefig('data_exploration.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def preprocess_data(self):
        """è³‡æ–™é è™•ç†"""
        print("\næ­¥é©Ÿ 3: è³‡æ–™é è™•ç†")
        print("=" * 50)
        
        # åˆ†é›¢ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸
        X = self.df.drop('target', axis=1)
        y = self.df['target']
        
        # åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # æ¨™æº–åŒ–ç‰¹å¾µ
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        print(f"è¨“ç·´é›†å¤§å°: {self.X_train.shape}")
        print(f"æ¸¬è©¦é›†å¤§å°: {self.X_test.shape}")
        print(f"è¨“ç·´é›†ç›®æ¨™åˆ†å¸ƒ: {pd.Series(self.y_train).value_counts().to_dict()}")
        print(f"æ¸¬è©¦é›†ç›®æ¨™åˆ†å¸ƒ: {pd.Series(self.y_test).value_counts().to_dict()}")
        
    def train_model(self):
        """è¨“ç·´XGBoostæ¨¡å‹"""
        print("\næ­¥é©Ÿ 4: è¨“ç·´XGBoostæ¨¡å‹")
        print("=" * 50)
        
        # åŸºæœ¬XGBoostæ¨¡å‹
        print("è¨“ç·´åŸºæœ¬XGBoostæ¨¡å‹...")
        self.model = xgb.XGBClassifier(
            objective='binary:logistic',
            random_state=42,
            eval_metric='logloss'
        )
        
        self.model.fit(self.X_train_scaled, self.y_train)
        
        # åŸºæœ¬é æ¸¬
        y_pred = self.model.predict(self.X_test_scaled)
        basic_accuracy = accuracy_score(self.y_test, y_pred)
        print(f"åŸºæœ¬æ¨¡å‹æº–ç¢ºç‡: {basic_accuracy:.4f}")
        
        # è¶…åƒæ•¸èª¿å„ª
        print("\né€²è¡Œè¶…åƒæ•¸èª¿å„ª...")
        param_grid = {
            'max_depth': [3, 4, 5],
            'learning_rate': [0.01, 0.1, 0.2],
            'n_estimators': [100, 200, 300],
            'subsample': [0.8, 0.9, 1.0]
        }
        
        grid_search = GridSearchCV(
            xgb.XGBClassifier(objective='binary:logistic', random_state=42, eval_metric='logloss'),
            param_grid,
            cv=5,
            scoring='accuracy',
            n_jobs=-1,
            verbose=1
        )
        
        grid_search.fit(self.X_train_scaled, self.y_train)
        
        # ä½¿ç”¨æœ€ä½³åƒæ•¸çš„æ¨¡å‹
        self.best_model = grid_search.best_estimator_
        print(f"\næœ€ä½³åƒæ•¸: {grid_search.best_params_}")
        print(f"æœ€ä½³äº¤å‰é©—è­‰åˆ†æ•¸: {grid_search.best_score_:.4f}")
        
    def evaluate_model(self):
        """æ¨¡å‹è©•ä¼°"""
        print("\næ­¥é©Ÿ 5: æ¨¡å‹è©•ä¼°")
        print("=" * 50)
        
        # é æ¸¬
        y_pred = self.best_model.predict(self.X_test_scaled)
        y_pred_proba = self.best_model.predict_proba(self.X_test_scaled)[:, 1]
        
        # æº–ç¢ºç‡
        accuracy = accuracy_score(self.y_test, y_pred)
        print(f"æ¸¬è©¦é›†æº–ç¢ºç‡: {accuracy:.4f}")
        
        # åˆ†é¡å ±å‘Š
        print("\nåˆ†é¡å ±å‘Š:")
        print(classification_report(self.y_test, y_pred))
        
        # äº¤å‰é©—è­‰åˆ†æ•¸
        cv_scores = cross_val_score(self.best_model, self.X_train_scaled, self.y_train, cv=5)
        print(f"\n5æŠ˜äº¤å‰é©—è­‰åˆ†æ•¸: {cv_scores}")
        print(f"å¹³å‡CVåˆ†æ•¸: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        
        # è¦–è¦ºåŒ–çµæœ
        self.visualize_results(y_pred, y_pred_proba)
        
    def visualize_results(self, y_pred, y_pred_proba):
        """è¦–è¦ºåŒ–çµæœ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # æ··æ·†çŸ©é™£
        cm = confusion_matrix(self.y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])
        axes[0,0].set_title('æ··æ·†çŸ©é™£')
        axes[0,0].set_xlabel('é æ¸¬å€¼')
        axes[0,0].set_ylabel('å¯¦éš›å€¼')
        
        # ç‰¹å¾µé‡è¦æ€§
        feature_importance = self.best_model.feature_importances_
        feature_names = self.df.columns[:-1]  # æ’é™¤targetæ¬„ä½
        
        # é¸æ“‡å‰15å€‹æœ€é‡è¦çš„ç‰¹å¾µ
        top_indices = np.argsort(feature_importance)[-15:]
        top_features = [feature_names[i] for i in top_indices]
        top_importance = feature_importance[top_indices]
        
        axes[0,1].barh(range(len(top_features)), top_importance)
        axes[0,1].set_yticks(range(len(top_features)))
        axes[0,1].set_yticklabels(top_features)
        axes[0,1].set_title('å‰15å€‹é‡è¦ç‰¹å¾µ')
        axes[0,1].set_xlabel('é‡è¦æ€§åˆ†æ•¸')
        
        # é æ¸¬æ©Ÿç‡åˆ†å¸ƒ
        axes[1,0].hist(y_pred_proba[self.y_test == 0], bins=20, alpha=0.7, label='æƒ¡æ€§ (0)', color='red')
        axes[1,0].hist(y_pred_proba[self.y_test == 1], bins=20, alpha=0.7, label='è‰¯æ€§ (1)', color='blue')
        axes[1,0].set_title('é æ¸¬æ©Ÿç‡åˆ†å¸ƒ')
        axes[1,0].set_xlabel('é æ¸¬æ©Ÿç‡')
        axes[1,0].set_ylabel('é »ç‡')
        axes[1,0].legend()
        
        # ROCæ›²ç·š
        from sklearn.metrics import roc_curve, auc
        fpr, tpr, _ = roc_curve(self.y_test, y_pred_proba)
        roc_auc = auc(fpr, tpr)
        
        axes[1,1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROCæ›²ç·š (AUC = {roc_auc:.2f})')
        axes[1,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        axes[1,1].set_xlim([0.0, 1.0])
        axes[1,1].set_ylim([0.0, 1.05])
        axes[1,1].set_xlabel('å½é™½æ€§ç‡')
        axes[1,1].set_ylabel('çœŸé™½æ€§ç‡')
        axes[1,1].set_title('ROCæ›²ç·š')
        axes[1,1].legend(loc="lower right")
        
        plt.tight_layout()
        plt.savefig('model_evaluation.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def save_model(self):
        """å„²å­˜æ¨¡å‹"""
        print("\næ­¥é©Ÿ 6: å„²å­˜æ¨¡å‹")
        print("=" * 50)
        
        # å„²å­˜XGBoostæ¨¡å‹
        self.best_model.save_model('xgboost_model.json')
        print("æ¨¡å‹å·²å„²å­˜ç‚º 'xgboost_model.json'")
        
        # å„²å­˜é è™•ç†å™¨
        import joblib
        joblib.dump(self.scaler, 'scaler.pkl')
        print("æ¨™æº–åŒ–å™¨å·²å„²å­˜ç‚º 'scaler.pkl'")
        
    def run_complete_pipeline(self):
        """åŸ·è¡Œå®Œæ•´çš„æ©Ÿå™¨å­¸ç¿’æµç¨‹"""
        print("ğŸš€ é–‹å§‹XGBoostæ©Ÿå™¨å­¸ç¿’å°ˆæ¡ˆ")
        print("=" * 60)
        
        self.load_data()
        self.explore_data()
        self.preprocess_data()
        self.train_model()
        self.evaluate_model()
        self.save_model()
        
        print("\nâœ… XGBoostå°ˆæ¡ˆå®Œæˆï¼")
        print("=" * 60)
        print("ç”Ÿæˆçš„æª”æ¡ˆ:")
        print("- data_exploration.png: è³‡æ–™æ¢ç´¢è¦–è¦ºåŒ–")
        print("- model_evaluation.png: æ¨¡å‹è©•ä¼°çµæœ")
        print("- xgboost_model.json: è¨“ç·´å¥½çš„XGBoostæ¨¡å‹")
        print("- scaler.pkl: è³‡æ–™æ¨™æº–åŒ–å™¨")

if __name__ == "__main__":
    # å»ºç«‹ä¸¦åŸ·è¡ŒXGBoostå°ˆæ¡ˆ
    project = XGBoostProject()
    project.run_complete_pipeline()