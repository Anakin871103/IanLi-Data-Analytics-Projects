## -------------------------------------------------------------------------------------------
# è¼‰å…¥å¿…è¦å¥—ä»¶
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib # æ–°å¢ joblib ä»¥ä¾¿å„²å­˜ GridSearch çš„æœ€ä½³æ¨¡å‹
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import roc_curve, auc
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')
## -------------------------------------------------------------------------------------------
# è¨­å®šä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

# å…¨åŸŸè®Šæ•¸åˆå§‹åŒ– (å–ä»£ Class å…§çš„ self è®Šæ•¸)
# é€™äº›è®Šæ•¸å°‡åœ¨å„å€‹æ­¥é©Ÿä¸­è¢«è³¦å€¼å’Œä½¿ç”¨
df = None
X_train_scaled = None
X_test_scaled = None
y_train = None
y_test = None
scaler = StandardScaler()
best_model = None

# ==============================================================================
# æ­¥é©Ÿ 1: è¼‰å…¥è³‡æ–™é›†
# ==============================================================================
def load_data():
    """è¼‰å…¥è³‡æ–™é›† - ä½¿ç”¨ä¹³ç™Œè³‡æ–™é›†ä½œç‚ºç¯„ä¾‹"""
    global df
    print("æ­¥é©Ÿ 1: è¼‰å…¥è³‡æ–™é›†")
    print("=" * 50)
    
    # è¼‰å…¥ä¹³ç™Œè³‡æ–™é›† (Wisconsin Breast Cancer Dataset)
    data = load_breast_cancer()
    df = pd.DataFrame(data.data, columns=data.feature_names)
    df['target'] = data.target
    
    print(f"è³‡æ–™é›†å½¢ç‹€: {df.shape}")
    print(f"ç‰¹å¾µæ•¸é‡: {len(data.feature_names)}")
    print(f"é¡åˆ¥åˆ†å¸ƒ:")
    print(df['target'].value_counts())
    print("\nå‰5ç­†è³‡æ–™:")
    print(df.head())
    print(df)
    return df

# ==============================================================================
# æ­¥é©Ÿ 2: è³‡æ–™æ¢ç´¢èˆ‡è¦–è¦ºåŒ–
# ==============================================================================
def explore_data(df):
    """è³‡æ–™æ¢ç´¢èˆ‡è¦–è¦ºåŒ–"""
    print("\næ­¥é©Ÿ 2: è³‡æ–™æ¢ç´¢èˆ‡è¦–è¦ºåŒ–")
    print("=" * 50)
    
    # åŸºæœ¬çµ±è¨ˆè³‡è¨Š
    print("åŸºæœ¬çµ±è¨ˆè³‡è¨Š:")
    print(df.describe())
    
    # æª¢æŸ¥ç¼ºå¤±å€¼
    print(f"\nç¼ºå¤±å€¼æ•¸é‡: {df.isnull().sum().sum()}")
    
    # è¦–è¦ºåŒ–
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # ç›®æ¨™è®Šæ•¸åˆ†å¸ƒ
    df['target'].value_counts().plot(kind='bar', ax=axes[0,0])
    axes[0,0].set_title('ç›®æ¨™è®Šæ•¸åˆ†å¸ƒ')
    axes[0,0].set_xlabel('é¡åˆ¥ (0: æƒ¡æ€§, 1: è‰¯æ€§)')
    
    # ç‰¹å¾µç›¸é—œæ€§ç†±åŠ›åœ– (é¸æ“‡å‰10å€‹ç‰¹å¾µ)
    # 
    corr_matrix = df.iloc[:, :10].corr()
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=axes[0,1], fmt=".2f")
    axes[0,1].set_title('ç‰¹å¾µç›¸é—œæ€§ç†±åŠ›åœ– (å‰10å€‹ç‰¹å¾µ)')
    
    # ç‰¹å¾µåˆ†å¸ƒ (é¸æ“‡å¹¾å€‹é‡è¦ç‰¹å¾µ)
    important_features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area']
    for i, feature in enumerate(important_features[:2]):
        axes[1,i].hist(df[feature], bins=30, alpha=0.7)
        axes[1,i].set_title(f'{feature} åˆ†å¸ƒ')
    
    plt.tight_layout()
    plt.savefig('data_exploration.png', dpi=300, bbox_inches='tight')
    plt.show()

# ==============================================================================
# æ­¥é©Ÿ 3: è³‡æ–™é è™•ç†
# ==============================================================================
def preprocess_data(df):
    """è³‡æ–™é è™•ç†"""
    global X_train_scaled, X_test_scaled, y_train, y_test, scaler
    print("\næ­¥é©Ÿ 3: è³‡æ–™é è™•ç†")
    print("=" * 50)
    
    # åˆ†é›¢ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸
    X = df.drop('target', axis=1)
    y = df['target']
    
    # åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†
    # X_train, X_test, y_train, y_test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # æ¨™æº–åŒ–ç‰¹å¾µ
    scaler.fit(X_train)
    X_train_scaled = scaler.transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"è¨“ç·´é›†å¤§å°: {X_train.shape}")
    print(f"æ¸¬è©¦é›†å¤§å°: {X_test.shape}")
    print(f"è¨“ç·´é›†ç›®æ¨™åˆ†å¸ƒ: {pd.Series(y_train).value_counts().to_dict()}")
    print(f"æ¸¬è©¦é›†ç›®æ¨™åˆ†å¸ƒ: {pd.Series(y_test).value_counts().to_dict()}")
    
    # å‡½æ•¸å›å‚³æ‰€æœ‰å¿…è¦çš„è®Šæ•¸
    return X_train_scaled, X_test_scaled, y_train, y_test, scaler

# ==============================================================================
# æ­¥é©Ÿ 4: è¨“ç·´XGBoostæ¨¡å‹
# ==============================================================================
def train_model(X_train_scaled, y_train, X_test_scaled, y_test):
    """è¨“ç·´XGBoostæ¨¡å‹ä¸¦é€²è¡Œè¶…åƒæ•¸èª¿å„ª"""
    global best_model
    print("\næ­¥é©Ÿ 4: è¨“ç·´XGBoostæ¨¡å‹")
    print("=" * 50)
    
    # åˆå§‹æ¨¡å‹ (ç”¨æ–¼æ¯”è¼ƒ)
    model = xgb.XGBClassifier(
        objective='binary:logistic',
        random_state=42,
        eval_metric='logloss'
    )
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    basic_accuracy = accuracy_score(y_test, y_pred)
    print(f"åŸºæœ¬æ¨¡å‹æº–ç¢ºç‡: {basic_accuracy:.4f}")
    
    # è¶…åƒæ•¸èª¿å„ª
    print("\né€²è¡Œè¶…åƒæ•¸èª¿å„ª...")
    param_grid = {
        'max_depth': [3, 4, 5],
        'learning_rate': [0.01, 0.1, 0.2],
        'n_estimators': [100, 200, 300],
        'subsample': [0.8, 0.9, 1.0]
    }
    
    grid_search = GridSearchCV(
        xgb.XGBClassifier(objective='binary:logistic', random_state=42, eval_metric='logloss'),
        param_grid,
        cv=5,
        scoring='accuracy',
        n_jobs=-1,
        verbose=1
    )
    
    grid_search.fit(X_train_scaled, y_train)
    
    # ä½¿ç”¨æœ€ä½³åƒæ•¸çš„æ¨¡å‹
    best_model = grid_search.best_estimator_
    print(f"\næœ€ä½³åƒæ•¸: {grid_search.best_params_}")
    print(f"æœ€ä½³äº¤å‰é©—è­‰åˆ†æ•¸: {grid_search.best_score_:.4f}")
    
    return best_model

# ==============================================================================
# æ­¥é©Ÿ 5: æ¨¡å‹è©•ä¼°
# ==============================================================================
def evaluate_model(best_model, X_train_scaled, y_train, X_test_scaled, y_test, df):
    """æ¨¡å‹è©•ä¼°èˆ‡è¦–è¦ºåŒ–"""
    print("\næ­¥é©Ÿ 5: æ¨¡å‹è©•ä¼°")
    print("=" * 50)
    
    # é æ¸¬
    y_pred = best_model.predict(X_test_scaled)
    y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]
    
    # æº–ç¢ºç‡
    accuracy = accuracy_score(y_test, y_pred)
    print(f"æ¸¬è©¦é›†æº–ç¢ºç‡: {accuracy:.4f}")
    
    # åˆ†é¡å ±å‘Š
    print("\nåˆ†é¡å ±å‘Š:")
    print(classification_report(y_test, y_pred))
    
    # äº¤å‰é©—è­‰åˆ†æ•¸ (åœ¨è¨“ç·´é›†ä¸Šé€²è¡Œ)
    cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=5)
    print(f"\n5æŠ˜äº¤å‰é©—è­‰åˆ†æ•¸: {cv_scores}")
    print(f"å¹³å‡CVåˆ†æ•¸: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    
    # è¦–è¦ºåŒ–çµæœ
    visualize_results(best_model, X_test_scaled, y_test, y_pred, y_pred_proba, df)

# ==============================================================================
# æ­¥é©Ÿ 5.1: è¦–è¦ºåŒ–çµæœ
# ==============================================================================
def visualize_results(best_model, X_test_scaled, y_test, y_pred, y_pred_proba, df):
    """è¦–è¦ºåŒ–çµæœ"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # æ··æ·†çŸ©é™£
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])
    axes[0,0].set_title('æ··æ·†çŸ©é™£')
    axes[0,0].set_xlabel('é æ¸¬å€¼')
    axes[0,0].set_ylabel('å¯¦éš›å€¼')
    # 
    
    # ç‰¹å¾µé‡è¦æ€§
    feature_importance = best_model.feature_importances_
    feature_names = df.columns[:-1] # æ’é™¤targetæ¬„ä½
    
    # é¸æ“‡å‰15å€‹æœ€é‡è¦çš„ç‰¹å¾µ
    top_indices = np.argsort(feature_importance)[-15:]
    top_features = [feature_names[i] for i in top_indices]
    top_importance = feature_importance[top_indices]
    
    axes[0,1].barh(range(len(top_features)), top_importance)
    axes[0,1].set_yticks(range(len(top_features)))
    axes[0,1].set_yticklabels(top_features)
    axes[0,1].set_title('å‰15å€‹é‡è¦ç‰¹å¾µ')
    axes[0,1].set_xlabel('é‡è¦æ€§åˆ†æ•¸')
    
    # é æ¸¬æ©Ÿç‡åˆ†å¸ƒ
    axes[1,0].hist(y_pred_proba[y_test == 0], bins=20, alpha=0.7, label='æƒ¡æ€§ (0)', color='red')
    axes[1,0].hist(y_pred_proba[y_test == 1], bins=20, alpha=0.7, label='è‰¯æ€§ (1)', color='blue')
    axes[1,0].set_title('é æ¸¬æ©Ÿç‡åˆ†å¸ƒ')
    axes[1,0].set_xlabel('é æ¸¬æ©Ÿç‡')
    axes[1,0].set_ylabel('é »ç‡')
    axes[1,0].legend()
    
    # ROCæ›²ç·š
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    
    axes[1,1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROCæ›²ç·š (AUC = {roc_auc:.2f})')
    axes[1,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    axes[1,1].set_xlim([0.0, 1.0])
    axes[1,1].set_ylim([0.0, 1.05])
    axes[1,1].set_xlabel('å½é™½æ€§ç‡')
    axes[1,1].set_ylabel('çœŸé™½æ€§ç‡')
    axes[1,1].set_title('ROCæ›²ç·š')
    axes[1,1].legend(loc="lower right")
    
    plt.tight_layout()
    plt.savefig('model_evaluation.png', dpi=300, bbox_inches='tight')
    plt.show()

# ==============================================================================
# æ­¥é©Ÿ 6: å„²å­˜æ¨¡å‹
# ==============================================================================
def save_model(best_model, scaler):
    """å„²å­˜æ¨¡å‹ (ä½¿ç”¨ joblib å„²å­˜æœ€ä½³ä¼°è¨ˆå™¨)"""
    print("\næ­¥é©Ÿ 6: å„²å­˜æ¨¡å‹")
    print("=" * 50)
    
    # å„²å­˜ GridSearchCV å¾—åˆ°çš„æœ€ä½³æ¨¡å‹ (ä½¿ç”¨ joblib æ¨™æº–æ–¹æ³•)
    # æª”æ¡ˆåç¨±æ”¹ç‚º .joblib ä»¥ç¬¦åˆæ…£ä¾‹
    model_filename = 'xgboost_grid_search_model.joblib'
    joblib.dump(best_model, model_filename)
    print(f"âœ… è¨“ç·´å¥½çš„æ¨¡å‹å·²å„²å­˜ç‚º '{model_filename}' (ä½¿ç”¨ joblib)ã€‚")
    
    # å„²å­˜é è™•ç†å™¨
    scaler_filename = 'scaler.pkl'
    joblib.dump(scaler, scaler_filename)
    print(f"âœ… æ¨™æº–åŒ–å™¨å·²å„²å­˜ç‚º '{scaler_filename}'ã€‚")

# ==============================================================================
# åŸ·è¡Œä¸»è¦æµç¨‹
# ==============================================================================
if __name__ == "__main__":
    
    print("ğŸš€ é–‹å§‹XGBoostæ©Ÿå™¨å­¸ç¿’å°ˆæ¡ˆ")
    print("=" * 60)
    
    # 1. è¼‰å…¥è³‡æ–™
    df_data = load_data()
    
    # 2. è³‡æ–™æ¢ç´¢
    explore_data(df_data)
    
    # 3. è³‡æ–™é è™•ç† (å›å‚³åˆ†å‰²å’Œæ¨™æº–åŒ–å¾Œçš„è³‡æ–™)
    X_train_scaled, X_test_scaled, y_train, y_test, scaler = preprocess_data(df_data)
    
    # 4. è¨“ç·´æ¨¡å‹
    best_model = train_model(X_train_scaled, y_train, X_test_scaled, y_test)
    
    # 5. æ¨¡å‹è©•ä¼°
    # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ df_data æ˜¯ç‚ºäº†åœ¨ visualize_results å…§å–å¾—ç‰¹å¾µåç¨±
    evaluate_model(best_model, X_train_scaled, y_train, X_test_scaled, y_test, df_data)
    
    # 6. å„²å­˜æ¨¡å‹
    save_model(best_model, scaler)
    
    print("\nâœ… XGBoostå°ˆæ¡ˆå®Œæˆï¼")
    print("=" * 60)
    print("ç”Ÿæˆçš„æª”æ¡ˆ:")
    print("- data_exploration.png: è³‡æ–™æ¢ç´¢è¦–è¦ºåŒ–")
    print("- model_evaluation.png: æ¨¡å‹è©•ä¼°çµæœ")
    print("- xgboost_grid_search_model.joblib: è¨“ç·´å¥½çš„XGBoostæœ€ä½³æ¨¡å‹ (Joblibæ ¼å¼)")
    print("- scaler.pkl: è³‡æ–™æ¨™æº–åŒ–å™¨")